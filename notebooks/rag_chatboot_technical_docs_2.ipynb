{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Document loading and splitting\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Tokenizer for the text processing\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Embeddings and vector storage\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "import faiss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 14:47:29,983 - INFO - The document has been loaded successfully. Total number of pages: 459.\n",
      "2024-10-22 14:47:31,798 - INFO - The document has been split into 1391 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Path to the document \n",
    "file_path = \"../data/raw/TA-9-2024-0138_EN.pdf\" \n",
    "\n",
    "# Function to split the document into chunks\n",
    "def split_document_into_chunks(file_path: str, chunk_size: int, tokenizer_name: str = EMBEDDING_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Load a document and split it into smaller chunks for processing.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the document file.\n",
    "        chunk_size (int): The maximum size of each chunk (number of tokens).\n",
    "        tokenizer_name (str): The name of the tokenizer to use for splitting the document.\n",
    "\n",
    "    Returns:\n",
    "        List of split document chunks.\n",
    "    \"\"\"\n",
    "    # Check if the document file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        logging.error(f\"The file '{file_path}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Load the document using PyPDFLoader\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "    logging.info(f\"The document has been loaded successfully. Total number of pages: {len(pages)}.\")\n",
    "\n",
    "    # Initialize a text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size * 0.1),  # 10% overlap between chunks\n",
    "        add_start_index=True, \n",
    "        strip_whitespace=True\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    logging.info(f\"The document has been split into {len(chunks)} chunks.\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Split the document into chunks\n",
    "chunk_size = 128  \n",
    "chunks = split_document_into_chunks(file_path, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 14:47:31,807 - INFO - Load pretrained SentenceTransformer: thenlper/gte-small\n",
      "2024-10-22 14:47:33,660 - INFO - Embedding model 'thenlper/gte-small' loaded successfully.\n",
      "2024-10-22 14:47:33,662 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2024-10-22 14:47:33,662 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2024-10-22 14:48:23,253 - INFO - Embeddings generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate embeddings for the document chunks\n",
    "def generate_embeddings(chunks):\n",
    "    \"\"\"\n",
    "    Generate embeddings for the given document chunks and store them using FAISS (uses the nearest neighbor search algorithm).\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): List of document chunks to generate embeddings for.\n",
    "        \n",
    "    Returns:\n",
    "        FAISS index containing the document embeddings.\n",
    "    \"\"\"\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_NAME,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cpu\"},  # Use CPU for embeddings\n",
    "        encode_kwargs={\"normalize_embeddings\": True}\n",
    "    )\n",
    "    logging.info(f\"Embedding model '{EMBEDDING_MODEL_NAME}' loaded successfully.\")\n",
    "\n",
    "    KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "        chunks, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "    )\n",
    "    logging.info(\"Embeddings generated successfully.\")\n",
    "\n",
    "    return KNOWLEDGE_VECTOR_DATABASE\n",
    "\n",
    "# Generate embeddings for the document chunks\n",
    "if chunks is not None:\n",
    "    KNOWLEDGE_VECTOR_DATABASE = generate_embeddings(chunks)\n",
    "else:\n",
    "    logging.error(\"Chunks not generated. Please check the document splitting process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbcdccfc735143fd8b6fb6d4bd2ea38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eduar\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\eduar\\.cache\\huggingface\\hub\\models--Qwen--Qwen2-0.5B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77cd79798f9470eaa531fcff8512a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187bea9c050348e9b06c2ae4e8e11816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eee4a2e0d6f4019a0b717db59744df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/Qwen/Qwen2-0.5B/resolve/main/tokenizer.json: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "2024-10-22 14:48:36,730 - WARNING - Error while downloading from https://huggingface.co/Qwen/Qwen2-0.5B/resolve/main/tokenizer.json: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4670fbc489a463cafe7de91f5613aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9266b0e235f432abb825b2df60997be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c1f872580e4d2e9d6cc47fd1d5d4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b42ec873e6e4e08aed896018dd368fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "READER_MODEL_NAME = \"Qwen/Qwen2-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "# Initialize the pipeline for text generation using the new model\n",
    "READER_LLM = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,  \n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.2,\n",
    "    return_full_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 14:51:26,780 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2024-10-22 14:51:26,780 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting retrieval for user_query='What is the purpose of this Regulation?'...\n",
      "\n",
      "==================================Top document==================================\n",
      "as required by this Regulation.\n",
      "==================================Metadata==================================\n",
      "{'source': '../data/raw/TA-9-2024-0138_EN.pdf', 'page': 74, 'start_index': 1570}\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What is the purpose of this Regulation?\"\n",
    "print(f\"\\nStarting retrieval for {user_query=}...\")\n",
    "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
    "print(\n",
    "    \"\\n==================================Top document==================================\"\n",
    ")\n",
    "print(retrieved_docs[0].page_content)\n",
    "print(\"==================================Metadata==================================\")\n",
    "print(retrieved_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=1000) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the original text of Document 5, the purpose of this Regulation is to improve the functioning of the internal market and promote the uptake of human-centric and trustworthy AI (AI). Specifically, it aims to ensure that these technological developments benefit society, protect individuals' privacy and security, respect human dignity, and uphold democratic values such as democracy, freedom of expression, and the right to information and access. Additionally, the regulation promotes transparency and accountability through clear and accessible guidelines and regulations aimed at preventing and addressing risks associated with the adoption and usage of AI technology.\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs_text = [\n",
    "    doc.page_content for doc in retrieved_docs\n",
    "]  # We only need the text of the documents\n",
    "context = \"\\nExtracted documents:\\n\"\n",
    "context += \"\".join(\n",
    "    [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)]\n",
    ")\n",
    "\n",
    "final_prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_query}\n",
    "\"\"\"\n",
    "\n",
    "# Redact an answer\n",
    "generated_text = READER_LLM(final_prompt, truncation=True, max_new_tokens=1000)\n",
    "\n",
    "# print(\"\\nGenerated Text:\\n\", generated_text)\n",
    "\n",
    "# Assuming this is the relevant portion of the output\n",
    "output = generated_text[0]['generated_text']  # Extract the generated text from the dictionary\n",
    "\n",
    "# Step 1: Split by the '<|im_start|>assistant' to find where the assistant's answer begins\n",
    "answer_start = output.split('<|im_start|>assistant')\n",
    "\n",
    "# Step 2: Extract the first occurrence of the assistant's response\n",
    "if len(answer_start) > 1:\n",
    "    first_answer = answer_start[1].split('<|im_end|>')[0].strip()  # Take the first answer and strip extra spaces\n",
    "\n",
    "print(first_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
