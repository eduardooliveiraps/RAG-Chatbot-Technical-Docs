{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from transformers import AutoModelForCausalLM, pipeline, AutoTokenizer\n",
    "import faiss \n",
    "import transformers\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import streamlit as st\n",
    "from streamlit_chat import message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
    "READER_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "FAISS_INDEX_PATH = \"embeddings/knowledge_vector_database.faiss\"\n",
    "PDF_FILE_PATH = \"data/raw/TA-9-2024-0138_EN.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to split the document into chunks\n",
    "def split_document_into_chunks(file_path: str, chunk_size: int, tokenizer_name: str = EMBEDDING_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Load a document and split it into smaller chunks for processing.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the document file.\n",
    "        chunk_size (int): The maximum size of each chunk (number of tokens).\n",
    "        tokenizer_name (str): The name of the tokenizer to use for splitting the document.\n",
    "\n",
    "    Returns:\n",
    "        List of split document chunks.\n",
    "    \"\"\"\n",
    "    # Check if the document file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        logging.error(f\"The file '{file_path}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Load the document using PyPDFLoader\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "    logging.info(f\"The document has been loaded successfully. Total number of pages: {len(pages)}.\")\n",
    "\n",
    "    # Initialize a text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size * 0.1),  # 10% overlap between chunks\n",
    "        add_start_index=True, \n",
    "        strip_whitespace=True\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    logging.info(f\"The document has been split into {len(chunks)} chunks.\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings for the document chunks\n",
    "def generate_embeddings(chunks: list):\n",
    "    \"\"\"\n",
    "    Generate embeddings for the given document chunks and store them using FAISS (uses the nearest neighbor search algorithm).\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): List of document chunks to generate embeddings for.\n",
    "        \n",
    "    Returns:\n",
    "        FAISS index containing the document embeddings.\n",
    "    \"\"\"\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_NAME,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cpu\"},  # Use CPU for embeddings\n",
    "        encode_kwargs={\"normalize_embeddings\": True}\n",
    "    )\n",
    "    logging.info(f\"Embedding model '{EMBEDDING_MODEL_NAME}' loaded successfully.\")\n",
    "\n",
    "    KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "        chunks, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "    )\n",
    "    logging.info(\"Embeddings generated successfully.\")\n",
    "\n",
    "    return KNOWLEDGE_VECTOR_DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the entire knowledge vector database to a file\n",
    "def save_knowledge_vector_database(knowledge_vector_database, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(knowledge_vector_database, f)\n",
    "    logging.info(f\"Knowledge vector database saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 07:16:07,500 - INFO - The document has been loaded successfully. Total number of pages: 459.\n",
      "2024-10-30 07:16:09,268 - INFO - The document has been split into 814 chunks.\n",
      "2024-10-30 07:16:09,335 - INFO - Load pretrained SentenceTransformer: thenlper/gte-small\n",
      "2024-10-30 07:16:11,702 - INFO - Embedding model 'thenlper/gte-small' loaded successfully.\n",
      "2024-10-30 07:16:11,717 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2024-10-30 07:16:11,717 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2024-10-30 07:17:04,824 - INFO - Embeddings generated successfully.\n",
      "2024-10-30 07:17:04,981 - INFO - Knowledge vector database saved to embeddings/knowledge_vector_database.faiss\n"
     ]
    }
   ],
   "source": [
    "# Ensure the directory for saving the knowledge vector database exists\n",
    "os.makedirs(os.path.dirname(FAISS_INDEX_PATH), exist_ok=True)\n",
    "\n",
    "chunks = split_document_into_chunks(PDF_FILE_PATH, chunk_size=256)\n",
    "if chunks is not None:\n",
    "    # Generate embeddings for the document chunks\n",
    "    knowledge_vector_database = generate_embeddings(chunks)\n",
    "    # Save the entire knowledge vector database to a file\n",
    "    save_knowledge_vector_database(knowledge_vector_database, FAISS_INDEX_PATH)\n",
    "else:\n",
    "    logging.error(\"Failed to split the document into chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize the reader model\n",
    "def initialize_reader_model(model_name: str = READER_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Initialize the LLM model for text generation.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): The name of the model to use for the LLM.\n",
    "    \n",
    "    Returns:\n",
    "        A HuggingFace pipeline for text generation.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, device_map=\"auto\", torch_dtype=\"auto\")    \n",
    "\n",
    "    reader_llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.2,\n",
    "    return_full_text=False,\n",
    "    )\n",
    "    logging.info(f\"Reader LLM model '{model_name}' initialized successfully.\")\n",
    "    return reader_llm, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the entire knowledge vector database from a file\n",
    "def load_knowledge_vector_database(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            knowledge_vector_database = pickle.load(f)\n",
    "        logging.info(f\"Knowledge vector database loaded from {file_path}\")\n",
    "        return knowledge_vector_database\n",
    "    else:\n",
    "        logging.error(f\"Knowledge vector database file {file_path} does not exist.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve relevant documents from the knowledge base\n",
    "def retrieve_relevant_docs(query: str, knowledge_vector_database, k: int = 5):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant documents from the FAISS knowledge base.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        knowledge_vector_database: The FAISS knowledge base for retrieval.\n",
    "        k (int): The number of top documents to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple containing the retrieved documents and their combined text.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting retrieval for query: {query}\")\n",
    "    retrieved_docs = knowledge_vector_database.similarity_search(query=query, k=k)\n",
    "\n",
    "    retrieved_docs_text = [doc.page_content for doc in retrieved_docs]\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {i}:::\\n{doc}\\n\" for i, doc in enumerate(retrieved_docs_text)])\n",
    "\n",
    "    return retrieved_docs, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the final answer using the retrieved documents and LLM\n",
    "def generate_answer_from_docs(query: str, context: str, reader_llm, tokenizer, max_new_tokens=512):\n",
    "    \"\"\"\n",
    "    Generate an answer using the LLM based on the retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        context (str): The text of the retrieved documents.\n",
    "        reader_llm: The text generation pipeline (LLM).\n",
    "        tokenizer: The tokenizer for formatting the chat-based prompt.\n",
    "        max_new_tokens (int): Maximum number of tokens for the generated answer.\n",
    "\n",
    "    Returns:\n",
    "        The generated answer from the LLM.\n",
    "    \"\"\"\n",
    "    # Chat-style prompt for the model\n",
    "    prompt_in_chat_format = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
    "    },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Context:\n",
    "            {context}\n",
    "            ---\n",
    "            Now here is the question you need to answer:\n",
    "\n",
    "            Question: {query}\"\"\"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Apply the chat-style template using tokenizer (if needed)\n",
    "    rag_prompt_template = tokenizer.apply_chat_template(\n",
    "        prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Generate the final answer\n",
    "    generated_text = reader_llm(rag_prompt_template, truncation=True, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Process the generated text\n",
    "    answer = generated_text[0]['generated_text']\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 07:21:15,186 - INFO - Knowledge vector database loaded from embeddings/knowledge_vector_database.faiss\n",
      "2024-10-30 07:21:16,387 - INFO - Reader LLM model 'Qwen/Qwen2.5-1.5B-Instruct' initialized successfully.\n",
      "2024-10-30 07:21:16,396 - INFO - Starting retrieval for query: What is the purpose of this Regulation?\n",
      "2024-10-30 07:21:16,397 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2024-10-30 07:21:16,398 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The purpose of this Regulation includes protecting natural persons, fostering democratic principles, ensuring legal governance, promoting employment growth through innovative applications of artificial intelligence technology, encouraging sustainable practices like environmental conservation, and establishing trustworthiness standards across Europe's digital landscape. Additionally, the regulation aims at enhancing cybersecurity protections during its execution phases such as inspections, prosecutions, safeguarding against breaches, maintaining operational integrity regarding personal privacy rights, managing sensitive governmental matters involving public safety concerns, preserving internal organizational dynamics under national laws compliance requirements, and overseeing international collaborations where EU entities operate abroad via cross-border agreements aimed at upholding regulatory consistency worldwide.\n"
     ]
    }
   ],
   "source": [
    "# Load the knowledge vector database\n",
    "knowledge_vector_database = load_knowledge_vector_database(FAISS_INDEX_PATH)\n",
    "if knowledge_vector_database is None:\n",
    "    logging.error(\"Failed to load the knowledge vector database.\")\n",
    "    exit(1)\n",
    "\n",
    "# Initialize the reader model\n",
    "reader_llm, tokenizer = initialize_reader_model()\n",
    "\n",
    "# Define the query\n",
    "query = \"What is the purpose of this Regulation?\"\n",
    "\n",
    "# Retrieve relevant documents\n",
    "retrieved_docs, context = retrieve_relevant_docs(query, knowledge_vector_database)\n",
    "\n",
    "# Generate the answer\n",
    "answer = generate_answer_from_docs(query, context, reader_llm, tokenizer)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
