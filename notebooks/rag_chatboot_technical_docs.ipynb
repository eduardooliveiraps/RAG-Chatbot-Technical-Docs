{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Document loading and splitting\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Tokenizer for the text processing\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Embeddings and vector storage\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "import faiss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 11:26:21,336 - INFO - The document has been loaded successfully. Total number of pages: 459.\n",
      "2024-10-25 11:26:23,265 - INFO - The document has been split into 1391 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Path to the document \n",
    "file_path = \"../data/raw/TA-9-2024-0138_EN.pdf\" \n",
    "\n",
    "# Function to split the document into chunks\n",
    "def split_document_into_chunks(file_path: str, chunk_size: int, tokenizer_name: str = EMBEDDING_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Load a document and split it into smaller chunks for processing.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the document file.\n",
    "        chunk_size (int): The maximum size of each chunk (number of tokens).\n",
    "        tokenizer_name (str): The name of the tokenizer to use for splitting the document.\n",
    "\n",
    "    Returns:\n",
    "        List of split document chunks.\n",
    "    \"\"\"\n",
    "    # Check if the document file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        logging.error(f\"The file '{file_path}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Load the document using PyPDFLoader\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "    logging.info(f\"The document has been loaded successfully. Total number of pages: {len(pages)}.\")\n",
    "\n",
    "    # Initialize a text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size * 0.1),  # 10% overlap between chunks\n",
    "        add_start_index=True, \n",
    "        strip_whitespace=True\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    logging.info(f\"The document has been split into {len(chunks)} chunks.\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Split the document into chunks\n",
    "chunk_size = 128  \n",
    "chunks = split_document_into_chunks(file_path, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 11:26:23,303 - INFO - Load pretrained SentenceTransformer: thenlper/gte-small\n",
      "2024-10-25 11:26:25,126 - INFO - Embedding model 'thenlper/gte-small' loaded successfully.\n",
      "2024-10-25 11:26:25,132 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2024-10-25 11:26:25,133 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2024-10-25 11:27:14,913 - INFO - Embeddings generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate embeddings for the document chunks\n",
    "def generate_embeddings(chunks):\n",
    "    \"\"\"\n",
    "    Generate embeddings for the given document chunks and store them using FAISS (uses the nearest neighbor search algorithm).\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): List of document chunks to generate embeddings for.\n",
    "        \n",
    "    Returns:\n",
    "        FAISS index containing the document embeddings.\n",
    "    \"\"\"\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_NAME,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cpu\"},  # Use CPU for embeddings\n",
    "        encode_kwargs={\"normalize_embeddings\": True}\n",
    "    )\n",
    "    logging.info(f\"Embedding model '{EMBEDDING_MODEL_NAME}' loaded successfully.\")\n",
    "\n",
    "    KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "        chunks, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "    )\n",
    "    logging.info(\"Embeddings generated successfully.\")\n",
    "\n",
    "    return KNOWLEDGE_VECTOR_DATABASE\n",
    "\n",
    "# Generate embeddings for the document chunks\n",
    "if chunks is not None:\n",
    "    KNOWLEDGE_VECTOR_DATABASE = generate_embeddings(chunks)\n",
    "else:\n",
    "    logging.error(\"Chunks not generated. Please check the document splitting process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "READER_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "# Initialize the pipeline for text generation using the new model\n",
    "READER_LLM = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.2,\n",
    "    return_full_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 11:27:17,151 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2024-10-25 11:27:17,153 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting retrieval for user_query='What is the purpose of this Regulation?'...\n",
      "\n",
      "==================================Top document==================================\n",
      "as required by this Regulation.\n",
      "==================================Metadata==================================\n",
      "{'source': '../data/raw/TA-9-2024-0138_EN.pdf', 'page': 74, 'start_index': 1570}\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What is the purpose of this Regulation?\"\n",
    "print(f\"\\nStarting retrieval for {user_query=}...\")\n",
    "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
    "print(\n",
    "    \"\\n==================================Top document==================================\"\n",
    ")\n",
    "print(retrieved_docs[0].page_content)\n",
    "print(\"==================================Metadata==================================\")\n",
    "print(retrieved_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  The purpose of this Regulation is to improve the functioning of the internal market and promote the uptake of human-centric and trustworthy artificial intelligence (AI). It also aims at ensuring a high level of protection of health, safety, fundamental rights enshrined in the Charter, including democracy, the rule of law and environmental protection, against the harmful effects of AI systems in the Union and supports innovation.\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs_text = [\n",
    "    doc.page_content for doc in retrieved_docs\n",
    "]  # We only need the text of the documents\n",
    "context = \"\\nExtracted documents:\\n\"\n",
    "context += \"\".join(\n",
    "    [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)]\n",
    ")\n",
    "\n",
    "# Chat-style prompt for the model\n",
    "prompt_in_chat_format = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Context:\n",
    "        {context}\n",
    "        ---\n",
    "        Now here is the question you need to answer:\n",
    "\n",
    "        Question: {user_query}\"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Apply the chat-style template using tokenizer (if needed)\n",
    "rag_prompt_template = tokenizer.apply_chat_template(\n",
    "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Redact an answer\n",
    "generated_text = READER_LLM(prompt_in_chat_format, truncation=True, max_new_tokens=1000)\n",
    "extracted_text = generated_text[0]['generated_text']\n",
    "print(\"Answer: \", extracted_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
