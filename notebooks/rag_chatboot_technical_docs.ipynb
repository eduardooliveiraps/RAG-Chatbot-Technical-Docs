{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Document loading and splitting\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Tokenizer for the text processing\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Embeddings and vector storage\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "import faiss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 09:22:34,898 - INFO - The document has been loaded successfully. Total number of pages: 459.\n",
      "2024-10-22 09:22:38,792 - INFO - The document has been split into 1391 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Path to the document \n",
    "file_path = \"../data/raw/TA-9-2024-0138_EN.pdf\" \n",
    "\n",
    "# Function to split the document into chunks\n",
    "def split_document_into_chunks(file_path: str, chunk_size: int, tokenizer_name: str = EMBEDDING_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Load a document and split it into smaller chunks for processing.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the document file.\n",
    "        chunk_size (int): The maximum size of each chunk (number of tokens).\n",
    "        tokenizer_name (str): The name of the tokenizer to use for splitting the document.\n",
    "\n",
    "    Returns:\n",
    "        List of split document chunks.\n",
    "    \"\"\"\n",
    "    # Check if the document file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        logging.error(f\"The file '{file_path}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Load the document using PyPDFLoader\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "    logging.info(f\"The document has been loaded successfully. Total number of pages: {len(pages)}.\")\n",
    "\n",
    "    # Initialize a text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size * 0.1),  # 10% overlap between chunks\n",
    "        add_start_index=True, \n",
    "        strip_whitespace=True\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    logging.info(f\"The document has been split into {len(chunks)} chunks.\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Split the document into chunks\n",
    "chunk_size = 128  \n",
    "chunks = split_document_into_chunks(file_path, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 09:22:38,808 - INFO - Load pretrained SentenceTransformer: thenlper/gte-small\n",
      "2024-10-22 09:22:40,946 - INFO - Embedding model 'thenlper/gte-small' loaded successfully.\n",
      "2024-10-22 09:22:40,961 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2024-10-22 09:22:40,961 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2024-10-22 09:23:59,172 - INFO - Embeddings generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate embeddings for the document chunks\n",
    "def generate_embeddings(chunks):\n",
    "    \"\"\"\n",
    "    Generate embeddings for the given document chunks and store them using FAISS (uses the nearest neighbor search algorithm).\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): List of document chunks to generate embeddings for.\n",
    "        \n",
    "    Returns:\n",
    "        FAISS index containing the document embeddings.\n",
    "    \"\"\"\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_NAME,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cpu\"},  # Use CPU for embeddings\n",
    "        encode_kwargs={\"normalize_embeddings\": True}\n",
    "    )\n",
    "    logging.info(f\"Embedding model '{EMBEDDING_MODEL_NAME}' loaded successfully.\")\n",
    "\n",
    "    KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "        chunks, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "    )\n",
    "    logging.info(\"Embeddings generated successfully.\")\n",
    "\n",
    "    return KNOWLEDGE_VECTOR_DATABASE\n",
    "\n",
    "# Generate embeddings for the document chunks\n",
    "if chunks is not None:\n",
    "    KNOWLEDGE_VECTOR_DATABASE = generate_embeddings(chunks)\n",
    "else:\n",
    "    logging.error(\"Chunks not generated. Please check the document splitting process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 09:24:00,823 - WARNING - Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "READER_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "# Initialize the pipeline for text generation using the new model\n",
    "READER_LLM = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,  \n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.2,\n",
    "    return_full_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 09:24:01,521 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2024-10-22 09:24:01,521 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting retrieval for user_query='What is the purpose of this Regulation?'...\n",
      "\n",
      "==================================Top document==================================\n",
      "as required by this Regulation.\n",
      "==================================Metadata==================================\n",
      "{'source': '../data/raw/TA-9-2024-0138_EN.pdf', 'page': 74, 'start_index': 1570}\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What is the purpose of this Regulation?\"\n",
    "print(f\"\\nStarting retrieval for {user_query=}...\")\n",
    "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
    "print(\n",
    "    \"\\n==================================Top document==================================\"\n",
    ")\n",
    "print(retrieved_docs[0].page_content)\n",
    "print(\"==================================Metadata==================================\")\n",
    "print(retrieved_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=1000) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Improve the functioning of the internal market and promote the uptake of human-centric and trustworthy artificial intelligence (AI)\n",
      "Additional Information: User generated context please try again\n",
      "\n",
      "To extract \"The purpose\" part directly related to your query:\n",
      "\n",
      "**Purpose:** Improve the functioning of the internal market and promote the uptake of human-centric and trustworthy artificial intelligence (AI).\n",
      "\n",
      "This summary captures what was extracted about the general provisions but focuses specifically on identifying the regulation's primary objective stated at its outset. If you have additional questions regarding these points or need further clarification, feel free to ask! \n",
      "\n",
      "### Explanation:\n",
      "\n",
      "- **Improvement of Internal Market Functioning**: Ensures smooth operation across different sectors without unnecessary barriers.\n",
      "  \n",
      "- **Promotion of Uptake of Human-Centric AI Systems**: Encourages adoption of AI technologies tailored towards ethical standards and benefits users' well-being more effectively than generic applications.\n",
      "\n",
      "These two aspects together encapsulate how the regulation aims to enhance overall performance through better integration of AI technology alongside safeguarding essential principles like privacy, security, and public interest considerations.\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs_text = [\n",
    "    doc.page_content for doc in retrieved_docs\n",
    "]  # We only need the text of the documents\n",
    "context = \"\\nExtracted documents:\\n\"\n",
    "context += \"\".join(\n",
    "    [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)]\n",
    ")\n",
    "\n",
    "final_prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_query}\n",
    "\"\"\"\n",
    "\n",
    "# Redact an answer\n",
    "generated_text = READER_LLM(final_prompt, truncation=True, max_new_tokens=1000)[0][\"generated_text\"]\n",
    "\n",
    "# Split the generated text at \"Answer:\"\n",
    "parts = generated_text.split(\"Answer:\")\n",
    "\n",
    "# Extract the main answer\n",
    "if len(parts) > 1:\n",
    "    answer = parts[1].split(\"\\n\")[0].strip()\n",
    "else:\n",
    "    answer = \"No answer found.\"\n",
    "\n",
    "# Extract additional information if available\n",
    "additional_info = \"\\n\".join(parts[1].split(\"\\n\")[1:]).strip() if len(parts) > 1 else \"\"\n",
    "\n",
    "print(\"Answer:\", answer)\n",
    "print(\"Additional Information:\", additional_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
